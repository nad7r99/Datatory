ğŸ§° Tech Stack

   Python

   Pandas (data cleaning & validation)

   PySpark (scalable ETL concepts)

   MySQL (Raw + DWH storage)

   SQL

 


ğŸ“‚ Data Source

   Input data comes from a CSV file

   Batch processing (no streaming)




ğŸ—„ï¸ Data Layers
   1ï¸âƒ£ Raw Layer

      Raw CSV data is ingested into the database without heavy transformations

   Purpose:

      Preserve original data

     Allow reprocessing if ETL fails

     Act as a learning replacement for a Data Lake


  2ï¸âƒ£ ETL Processing Layer
  
    ETL is performed using Pandas and PySpark:

    Data cleaning

    Data type corrections

    Column renaming

    Null handling

    Basic validation



  3ï¸âƒ£ Data Quality Checks

    The ETL process focuses on the following Data Quality dimensions:

    Completeness

    Consistency

    Validity

    Accuracy
  
    Uniqueness

    Timeliness

    Checks are applied at a basic level suitable for learning.



ğŸ§± Data Warehouse Design(OLAP)
â­ Star Schema

    The Data Warehouse is modeled using a Star Schema(1 fact table and 2 dimention table).

    Fact Table
      fact_Emp
      Monthly_salary (Fact)
      st_id (FK)
      date_id (FK)

    Dimension Tables

       dim_Emp
       Status
       Education
       Industry
       Location
       AI_Risk
       Age_Group
       Exp_years


    dim_date
      Date_Recorded

    âœ”ï¸ Surrogate Keys are used
    âœ”ï¸ Fact contains only metrics + FKs
    âœ”ï¸ Dimensions store descriptive attributes








   
